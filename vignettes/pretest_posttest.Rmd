---
title: "Pre-Test Post-Test Design"
output: rmarkdown::html_vignette
bibliography: bib.bib
vignette: >
  %\VignetteIndexEntry{Pre-Test Post-Test}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r MIDA, echo = FALSE,include = FALSE}
library(DesignLibrary)
library(knitr)
```

## Pre-Test Post-Test Design

Pre-test post-test designs are designs in which researchers estimate the change in outcomes before and after an intervention. These designs are often preferred to post-test-only designs (which simply compare outcomes between control and treatment group *after* treatment assignment), because they enable much more efficient estimation and more informed assessment of imbalance.

Nevertheless, baseline measurement often comes at a cost: when faced with budget constraints, researchers may be forced to decrease endline sample size in order to facilitate a baseline. Whether it is worth doing so often depends on how well the baseline will predict outcomes at endline.

Furthermore, there is much debate about how best to estimate treatment effects in such designs: when are researchers better off using  change scores versus conditioning on the baseline?

Below we consider the example of a pre-test post-test applied to a study that seeks to evaluate the effect of a family-planning program on the incidence of teenage pregnancy.

### Design Declaration

  - **M**odel: We define a population of size $N$, where effect at time $t = 1$ (pre-program) and $t = 2$ (post-program) are taken from a normal distribution of mean 0 and standard deviation smaller than 1. We assume pre- and post-test outcomes to be highly and positively correlated ($\rho = 0.5$). We also expect subjects to leave the study at a rate of 10%, meaning we do not observe post-treatment outcomes for a tenth of the sample.
  
- **I**nquiry: We wish to know the average effect of family pregnancy programs $Z$ on rates of teenage pregnancy. Formally: $E[Y(Z = 1) - Y(Z = 0) \mid t = 2]$, where $Z = 1$ denotes assignment to the program.

- **D**ata strategy: We observe the incidence of teenage pregnancy ($Y_i$) for individual $i$ for a sample of 100 individuals at time $t = 1$ (just prior to treatment) and at time $t = 2$ (a year after treatment). We randomly assign 50 out of 100 women between the ages of 15 and 19 to receive treatment. 

- **A**nswer strategy: We define three estimators. First, we estimate effects on the ``change score'': the dependent variable is defined as the difference between observed post- and pre-treatment outcomes. The second estimator treats only the post-treatment outcome as the dependent variable, but conditions on the pre-treatment outcome on the righthand side of the regression. Finally, we also look at effects when we only use post-test outcome measures, so as to evaluate the gain from using a baseline. 

```{r, eval = FALSE, code = get_design_code(pretest_posttest_designer())}

```

### Takeaways

```{r}
diagnosis <- diagnose_design(pretest_posttest_design)
```

```{r, echo=FALSE}
diagnosis_table <- get_diagnosands(diagnosis)[,c("estimand_label", "estimator_label",
                                                 "mean_estimand",
                                                 "mean_estimate", "se(mean_estimate)",
                                                 "bias", "se(bias)")]
kable(diagnosis_table,
      col.names = c("Estimand Label", "Estimator Label", "Mean Estimand",
                    "Mean Estimate", "SE(Mean Estimate)",
                    "Bias", "SE(Bias)"),
      digits = 2)
```

<!-- By running the diagnosis on our design, we observe that although the first two estimates are similar in magnitude and both unbiased, our "Change score" estimator (our first inquiry) is less efficient than the "Condition on pretest" estimator (our second inquiry) because we essentially "lose" information by defining an estimator as a difference. -->

<!-- Our "Posttest only" estimator is slightly biased. -->

<!-- How well these designs do depends on sample size, attrition rate, and ATE. -->


