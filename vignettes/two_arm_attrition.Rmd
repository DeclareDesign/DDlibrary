---
title: "Two-Arm Experiment with Attrition"
output: rmarkdown::html_vignette
bibliography: bib.bib
vignette: >
  %\VignetteIndexEntry{Two-Arm Experiment with Attrition}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r MIDA, echo = FALSE,include = FALSE}
library(DesignLibrary)
library(knitr)
```

You don't always get the data you want. Very often, individuals who would be relevant to your inquiry don't show up in the data: people refuse to answer surveys, data gets lost, collection activities are interrupted.

When data goes missing, two things happen. First, your power goes down because you have less data to work with relative to a study with complete data. Second, you have to worry about whether there exists any systematic relationship between missingness and the outcomes you are studying. If such a relationship exists, it can introduce bias. 

These features of attrition are fairly well-known. But how much attrition is too much attrition? How high does the correlation between the propensity to go missing and the outcome you care about have to be, in order to seriously jeapordize a study? Here, we declare a design that allows us to study such questions.

## Design Declaration

- **M**odel: 
    
    Our model of the world specifies a population of $N$ units that have three variables affected by the treatment: a response variable, $R_i$; our outcome of interest, $Y_i$, which is correlated with the response variable through $\rho$; and $Y^{obs}_i$, the *measured* version of the true $Y_i$, which is only observed when $R_i = 1$. 
  
- **I**nquiry: 
    
    We want to know the average of all units' differences in treated and untreated potential outcomes -- the average treatment effect on the outcome of interest: $E[Y_i(Z = 1) - Y_i(Z = 0)]$. But we also want to know the average treatment effect on reporting, $E[R_i(Z = 1) - R_i(Z = 0)]$, as well as the effect of the treatment among those who report, $E[Y_i(Z = 1) - Y_i(Z = 0) \mid R_i = 1]$.

- **D**ata strategy: 
    
    We randomly assign half of the units to treatment.

- **A**nswer strategy: 
    
    For $R_i$ and $Y^{obs}_i$, we subtract the mean of the control group's values  from the mean of the treatment group in order to estimate the average treatment effect. 

```{r, code = get_design_code(two_arm_attrition_designer()), eval=TRUE}
```

## Takeaways

```{r}
designs <- expand_design(designer = two_arm_attrition_designer, 
                         rho = c(0,.2,.8))
diagnoses <- diagnose_designs(designs, sims = 25)
```

```{r,eval = TRUE}
kable(reshape_diagnosis(diagnoses,select = c("Bias","Power")), digits = 2)
```

- The diagnosis illustrates that the effect on reporting can always be estimated with high power and no bias

- However, any strategy that conditions on $Y_i^{obs}$ is very biased, even for an estimand that is conditional on reporting. Even a small amount of correlation between missingness and outcomes can severely jeapordize inferences.









